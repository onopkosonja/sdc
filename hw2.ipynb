{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "ML разработчикам необходимо не только уметь обучать нейронные сети и генерировать новые идеи, но еще и уметь в страивать наработки в pipeline. В этом домашнем задании нам предстоит сделать из frustum детектора production ready (ну почти) решение, которое может работать на сырых данных.\n",
    "\n",
    "К сожалению, frustum-pointnet работает независимо для каждой 2D детекции. В этом домашнем задании вам предстоит написать обертку над frustum-pointnet, которая будет работать над целыми облаками. Вам также нужно будет воспользоваться 2D детектором, чтобы находить коробки на изображении.\n",
    "\n",
    "Во второй части задания вам нужно будет написать оценку качества работы вашего алгоритма, которая становится чуть сложнее, когда на сцене могут находится много объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1\n",
    "\n",
    "Ниже написан класс, который вам нужно реализовать. Чтобы воспользоваться предобученной сетью, позаимствуйте код из\n",
    "https://github.com/charlesq34/frustum-pointnets/blob/master/train/test.py\n",
    "\n",
    "Предобученные модели лежат здесь: https://shapenet.cs.stanford.edu/media/frustum_pointnets_snapshots.zip\n",
    "\n",
    "В частности, вам нужно модифицировать функцию `get_session_and_ops` - функция должна уметь работать без глобальных флагов. После этого посмотрите, как эта функция используется.\n",
    "Выход сети преобразуется в понятный формат в функции `write_detection_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"tensorflow==1.14\"\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from ssd import SSD\n",
    "sys.path.append(os.path.join(os.getcwd(), 'frustum_pointnets/train'))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'frustum_pointnets/train/test_'))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'frustum_pointnets/kitti'))\n",
    "import provider\n",
    "from test_ import inference, get_session_and_ops\n",
    "from kitti_util import Calibration\n",
    "from kitti_object import get_lidar_in_image_fov, kitti_object\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Detection = namedtuple('Detection', ['xyz', 'angle', 'lwh', 'confidence'])\n",
    "Scene = namedtuple('Scene', ['detections'])\n",
    "\n",
    "class PipelineDetector(object):\n",
    "    def __init__(self, ssd_detector, ssd_threshold, frustum_pointnet, \n",
    "                 frustum_batch_size, frustum_num_pts):\n",
    "        \n",
    "        self.ssd_detector = ssd_detector\n",
    "        self.ssd_threshold = ssd_threshold\n",
    "        \n",
    "        self.frustum_pointnet = frustum_pointnet\n",
    "        self.frustum_batch_size = frustum_batch_size\n",
    "        self.frustum_num_pts = frustum_num_pts\n",
    "        self.frustum_sess, self.frustum_ops = get_session_and_ops(self.frustum_batch_size, self.frustum_num_pts)\n",
    "  \n",
    "    def predict(self, velo_pts, image, calib):\n",
    "        # TODO: run 2D detector on the image\n",
    "        # TODO: extract bounding boxes with vehicle classes and filter them by ssd_threshold\n",
    "        detection = self.ssd_detector.predict(image)\n",
    "        vehicle_idx = np.where(detection['detection_classes'] == 1)\n",
    "        conf_idx = np.where(detection['detection_scores'] >= self.ssd_threshold)\n",
    "        final_idx = np.intersect1d(vehicle_idx, conf_idx)\n",
    "        bbox = detection['detection_boxes'][final_idx]    \n",
    "        detection_conf = detection['detection_scores'][final_idx]\n",
    "\n",
    "        # TODO: process lidar point cloud and construct frustum examples   \n",
    "        rect_pts = np.zeros_like(velo_pts)\n",
    "        rect_pts[:, :3] = calib.project_velo_to_rect(velo_pts[:, :3].copy())\n",
    "        rect_pts[:, 3] = velo_pts[:, 3]\n",
    "        \n",
    "        img_height, img_width, _= img.shape\n",
    "        _, img_pts, in_img_mask = get_lidar_in_image_fov(velo_pts[:, :3].copy(), calib, 0, 0, img_width, img_height, return_more=True)\n",
    "         \n",
    "        frustum_examples = []\n",
    "        frustum_angles = []\n",
    "        scene = Scene([])\n",
    "\n",
    "        for box in bbox:\n",
    "            box = (box.reshape((2, 2)) * image.shape[:2]).astype(int)\n",
    "            (ul_y, ul_x), (lr_y, lr_x)  = box \n",
    "            box_mask = (img_pts[:, 1] < lr_y) * (img_pts[:, 1] >= ul_y) * (img_pts[:, 0] < lr_x) * (img_pts[:, 0] >= ul_x) \n",
    "            \n",
    "            mask = in_img_mask & box_mask\n",
    "            rect_pts_masked = rect_pts[mask]            \n",
    "\n",
    "            if len(rect_pts_masked):\n",
    "                box2d_center = np.array([(ul_x + lr_x) / 2.0, (ul_y + ul_y) / 2.0])\n",
    "                uvdepth = np.zeros((1, 3))\n",
    "                uvdepth[0, :2] = box2d_center\n",
    "                uvdepth[0, 2] = 20 # some random depth\n",
    "                box2d_center_rect = calib.project_image_to_rect(uvdepth)\n",
    "                frustum_angle = -1 * np.arctan2(box2d_center_rect[0, 2], box2d_center_rect[0, 0])\n",
    "                frustum_angle += np.pi/2.0\n",
    "                \n",
    "                np.random.seed()\n",
    "                point_cloud = provider.rotate_pc_along_y(rect_pts_masked.copy(), frustum_angle)\n",
    "                idx = np.random.choice(len(point_cloud), size=self.frustum_num_pts, replace=True)\n",
    "                point_cloud = point_cloud[idx]\n",
    "                \n",
    "                frustum_angles.append(frustum_angle)\n",
    "                frustum_examples.append(point_cloud)\n",
    "            \n",
    "        if len(frustum_examples):\n",
    "            one_hot_batch = np.array([[1, 0, 0],] * len(frustum_examples)).reshape(-1, 3)        \n",
    "\n",
    "            #TODO: run frustum inference (use batch to accelerate inference per frame)\n",
    "            predictions = self.frustum_pointnet(self.frustum_sess, \n",
    "                                               self.frustum_ops, \n",
    "                                               np.array(frustum_examples), \n",
    "                                               one_hot_batch, \n",
    "                                               self.frustum_batch_size)\n",
    "            _, centers, heading_cls, heading_res, size_cls, size_res, _ = predictions\n",
    "\n",
    "            # TODO: construct Scene namedtuple and return it\n",
    "            for i, _ in enumerate(predictions):    \n",
    "                h, w, l, tx, ty, tz, ry = provider.from_prediction_to_label_format(centers[i], \n",
    "                                                                                   heading_cls[i], \n",
    "                                                                                   heading_res[i], \n",
    "                                                                                   size_cls[i], \n",
    "                                                                                   size_res[i], \n",
    "                                                                                   frustum_angles[i])\n",
    "                detection = Detection(xyz=np.array((tx, ty, tz)), angle=ry, lwh=np.array((l, w, h)), confidence=detection_conf[i])\n",
    "                scene.detections.append(detection)\n",
    "            return scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2\n",
    "\n",
    "Для оценки качества работы 3D детекторов обычно используется average precision. Как измерить precision и recall детектора?\n",
    "\n",
    "У каждой коробки детектора есть confidence. После того, как мы зафиксировали порог, у нас остается часть детекций.\n",
    "Давайте теперь посмотрим на сцену сверху: bird's eye view. Забудем про координату z.\n",
    "\n",
    "Далее мы можем посчитать IoU между всеми коробками ground truth и нашими детекциями.Давайте решим, что если IoU больше 0.7, то мы будем считать, что мы увидели gt коробку - относим эту детекцию к TP. Если gt не нашла пару - False Negative. Если детекция не нашла пару - False Positive.\n",
    "\n",
    "Ваша задача написать код подсчета метрики average precision построенного детектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_bbox(labels, calib, types=['Car', 'Van']):\n",
    "    bbox = []\n",
    "    for label in labels:\n",
    "        if label.type in types:\n",
    "            location_ref = np.array(label.t).reshape(1, -1) \n",
    "            location_velo = calib.project_ref_to_velo(location_ref).reshape(-1,)\n",
    "            length = label.l\n",
    "            width = label.w\n",
    "            angle = label.ry\n",
    "            poly = get_polygon(location_velo[:2], width, length, angle)\n",
    "            bbox.append(poly)\n",
    "    filtered_bbox = filter_by_distance(bbox)\n",
    "    return filtered_bbox\n",
    "\n",
    "def get_pred_bbox(detections, calib):\n",
    "    bbox = []\n",
    "    for det in detections:\n",
    "        location_ref = det.xyz.reshape(1, -1)\n",
    "        location_velo =  calib.project_ref_to_velo(location_ref).reshape(-1,)\n",
    "        length, width, _ = det.lwh\n",
    "        angle = det.angle\n",
    "        poly = get_polygon(location_velo[:2], width, length, angle)\n",
    "        bbox.append(poly)\n",
    "    filtered_bbox = filter_by_distance(bbox)\n",
    "    return filtered_bbox\n",
    "            \n",
    "def get_polygon(xy, width, length, angle):\n",
    "    polygon = Polygon([(-width/2, -length/2), (-width/2, length/2), (width/2, length/2), (width/2, -length/2)])\n",
    "    polygon = rotate(polygon, -angle, use_radians=True)\n",
    "    pts = np.array(polygon.exterior.coords)[:4]\n",
    "    pts += xy\n",
    "    return Polygon(pts)\n",
    "\n",
    "def filter_by_distance(bbox, max_dist=40):\n",
    "    centroids = [box.centroid for box in bbox]\n",
    "    dist_bbox = [bbox[i] for (i, c) in enumerate(centroids) if c.x**2 + c.y**2 <= max_dist **2 ]\n",
    "    return dist_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval_det.py\n",
    "\n",
    "def compute_iou(polygon1, polygon2):\n",
    "    return polygon1.intersection(polygon2).area / polygon1.union(polygon2).area\n",
    "\n",
    "def voc_ap(rec, prec):\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def eval_det_cls(pred, gt, ovthresh=0.7):\n",
    "    \"\"\" Generic functions to compute precision/recall for object detection\n",
    "        for a single class.\n",
    "        Input:\n",
    "            pred: map of {img_id: [(bbox, score)]} where bbox is numpy array\n",
    "            gt: map of {img_id: [bbox]}\n",
    "            ovthresh: scalar, iou threshold\n",
    "            use_07_metric: bool, if True use VOC07 11 point method\n",
    "        Output:\n",
    "            rec: numpy array of length nd\n",
    "            prec: numpy array of length nd\n",
    "            ap: scalar, average precision\n",
    "    \"\"\"\n",
    "    # construct gt objects\n",
    "    class_recs = {} # {img_id: {'bbox': bbox list, 'det': matched list}}\n",
    "    npos = 0\n",
    "    for img_id in gt.keys():\n",
    "        bbox = np.array(gt[img_id])\n",
    "        det = [False] * len(bbox)\n",
    "        npos += len(bbox)\n",
    "        class_recs[img_id] = {'bbox': bbox, 'det': det}\n",
    "    # pad empty list to all other imgids\n",
    "    for img_id in pred.keys():\n",
    "        if img_id not in gt:\n",
    "            class_recs[img_id] = {'bbox': np.array([]), 'det': []}\n",
    "\n",
    "    # construct dets\n",
    "    image_ids = []\n",
    "    confidence = []\n",
    "    BB = []\n",
    "    for img_id in pred.keys():\n",
    "        for box,score in pred[img_id]:\n",
    "            image_ids.append(img_id)\n",
    "            confidence.append(score)\n",
    "            BB.append(box)\n",
    "    confidence = np.array(confidence)\n",
    "    BB = np.array(BB) # (nd,4 or 8,3)\n",
    "\n",
    "    # sort by confidence\n",
    "    sorted_ind = np.argsort(-confidence)\n",
    "    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind]\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    nd = len(image_ids)\n",
    "    tp = np.zeros(nd)\n",
    "    fp = np.zeros(nd)\n",
    "    for d in range(nd):\n",
    "        R = class_recs[image_ids[d]]\n",
    "        bb = BB[d]\n",
    "        ovmax = -np.inf\n",
    "        BBGT = R['bbox']\n",
    "\n",
    "        if BBGT.size > 0:\n",
    "            # compute overlaps\n",
    "            for j in range(BBGT.shape[0]):\n",
    "                iou = compute_iou(bb, BBGT[j]) \n",
    "                if iou > ovmax:\n",
    "                    ovmax = iou\n",
    "                    jmax = j\n",
    "\n",
    "        #print d, ovmax\n",
    "        if ovmax > ovthresh:\n",
    "            if not R['det'][jmax]:\n",
    "                tp[d] = 1.\n",
    "                R['det'][jmax] = 1\n",
    "            else:\n",
    "                fp[d] = 1.\n",
    "        else:\n",
    "            fp[d] = 1.\n",
    "\n",
    "    # compute precision recall\n",
    "    fp = np.cumsum(fp)\n",
    "    tp = np.cumsum(tp)\n",
    "    rec = tp / float(npos)\n",
    "    # avoid divide by zero in case the first detection matches a difficult\n",
    "    # ground truth\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "    ap = voc_ap(rec, prec)\n",
    "\n",
    "    return rec, prec, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detections_and_gt(dataset_len):\n",
    "    ssd = SSD()\n",
    "    detector = PipelineDetector(ssd_detector=ssd, \n",
    "                                ssd_threshold=0.5, \n",
    "                                frustum_pointnet=inference, \n",
    "                                frustum_batch_size=1, \n",
    "                                frustum_num_pts=1024)\n",
    "    dataset = kitti_object(root_dir='./')\n",
    "    \n",
    "    det_bboxs = dict()\n",
    "    gt_bboxs = dict()\n",
    "    \n",
    "    for idx in tqdm(range(dataset_len)):\n",
    "        img = dataset.get_image(idx)\n",
    "        lidar = dataset.get_lidar(idx)\n",
    "        calib = dataset.get_calibration(idx)\n",
    "        labels = dataset.get_label_objects(idx)\n",
    "        detections = detector.predict(lidar, img, calib)\n",
    "        \n",
    "        if detections is not None:\n",
    "            detection = detections.detections\n",
    "        \n",
    "            det_bbox = get_pred_bbox(detections, calib)\n",
    "            gt_bbox = get_gt_bbox(labels, calib)\n",
    "\n",
    "            box_with_conf = []\n",
    "            for i, box in enumerate(det_bbox):\n",
    "                conf = detections[i].confidence\n",
    "                box_with_conf.append([box, conf])\n",
    "            det_bboxs[idx] = box_with_conf \n",
    "            gt_bboxs[idx] = gt_bbox\n",
    "    return det_bboxs, gt_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(glob.glob(os.path.join('training/calib', '*txt')))\n",
    "det_bboxs, gt_bboxs = get_detections_and_gt(dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, ap = eval_det_cls(det_bboxs, gt_bboxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
